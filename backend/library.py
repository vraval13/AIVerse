import requests
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAI
import google.generativeai as genai
from concurrent.futures import ThreadPoolExecutor, as_completed
import numpy as np
import faiss
import os
import assemblyai as aai
from moviepy.editor import VideoFileClip
from transformers import AutoTokenizer, AutoModel
import torch
from tqdm import tqdm
from langchain_groq import ChatGroq
from dotenv import dotenv_values
import glob
import shutil
from fastembed.embedding import FlagEmbedding as Embedding

fe_model = Embedding(model_name="BAAI/bge-base-en-v1.5")
# Load configuration from .env file
config = dotenv_values(".env") 


def embeder(text):

    embedding = fe_model.embed([text])
    import numpy as np 
    np_embed = np.array(list(embedding)[0])
    return np_embed


def data_give(index, search_query, chunks):
    """
    Given a FAISS index, search query, and corresponding text chunks, find the most relevant context,
    then generate and return a comprehensive answer using LLM (via ChatGroq).
    """
    search_vector = embeder(search_query)
    search_vector = np.array(search_vector).reshape(1, -1)
    distance, loc = index.search(search_vector, k=3)
    data_to_be_given = ""
    for i in loc[0]:
        data_to_be_given += chunks[i]
    prompt = (
        "You are given the following data as context: " + data_to_be_given +
        " now answer the following query on the basis of context: " + search_query +
        " The answer should be highly comprehensive."
    )
    groq_api_key = config['GROQ_API_KEY_LIB']
    llm = ChatGroq(groq_api_key=groq_api_key, model_name="llama3-70b-8192")
    return llm.invoke(prompt)


def process_files_in_directory(directory_path):
    """
    Process all supported files (PDFs and videos) in the directory, build embeddings,
    and create a FAISS index for fast similarity search.
    """
    local_chunks = []
    files = glob.glob(os.path.join(directory_path, "*"))
    for file_path in files:
        if file_path.endswith(".pdf"):
            print(f"Processing PDF: {file_path}")
            loader = PyPDFLoader(file_path)
            pages = loader.load()
            content = [page.page_content for page in pages]
            splitter = RecursiveCharacterTextSplitter(
                separators=["\n\n", "\n", ".", "!", "?", " "],
                chunk_size=2000,
                chunk_overlap=1
            )
            for page_content in content:
                local_chunks.extend(splitter.split_text(page_content))
        elif file_path.endswith((".mp4", ".avi", ".mov")):
            print(f"Processing Video: {file_path}")
            audio_path = "temp_audio.wav"
            video = VideoFileClip(file_path)
            video.audio.write_audiofile(audio_path)
            try:
                transcriber = aai.Transcriber()
                transcription = transcriber.transcribe(audio_path)
                text = transcription.text
                splitter = RecursiveCharacterTextSplitter(
                    separators=["\n\n", "\n", ".", "!", "?", " "],
                    chunk_size=2000,
                    chunk_overlap=1
                )
                local_chunks.extend(splitter.split_text(text))
            finally:
                if os.path.exists(audio_path):
                    os.remove(audio_path)
        else:
            print(f"Unsupported file type: {file_path}")
    embeddings = list(fe_model.embed(local_chunks))
    if not embeddings:
        raise ValueError("No valid embeddings generated from files.")
    vectors = np.array(embeddings)
    dim = vectors.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(vectors)
    return index, local_chunks

def give_vectors(index, search_query, chunks):
    """
    Retrieve the aggregated text chunks corresponding to the top similar vectors for a query.
    """
    search_vector = embeder(search_query)
    search_vector = np.array(search_vector).reshape(1, -1)
    distance, loc = index.search(search_vector, k=3)
    data_to_be_given = ""
    for i in loc[0]:
        data_to_be_given += chunks[i]
    return data_to_be_given

# Final class remains in the same format as before
class Librarian:
    def __init__(self):
        self.index = None        # FAISS index
        self.chunks = []         # Associated text chunks

    def add_material(self, path_to_material):
        """
        Process all files in the given directory and build the FAISS index and chunks.
        """
        self.index, self.chunks = process_files_in_directory(path_to_material)

    def query_material(self, prompt):
        """
        Query the material using the provided prompt. Returns an answer generated by the LLM.
        """
        if self.index is None:
            raise ValueError("No material has been added.")
        return data_give(self.index, prompt, self.chunks)

    def provide_vector(self, query):
        """
        Return the concatenated text chunks corresponding to the query.
        """
        if self.index is None:
            raise ValueError("No material has been added.")
        return give_vectors(self.index, query, self.chunks)

    def remover_material(self):
        """
        Remove indexed material and delete the 'content' folder if it exists.
        """
        if os.path.exists('content'):
            shutil.rmtree('content')
        self.index = None
        self.chunks = []

    def embedder(self, text):
        return embeder(text)
